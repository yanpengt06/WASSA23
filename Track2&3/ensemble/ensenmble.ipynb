{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from main import calculate_pearson, calculatePRF_MLabel\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_ensemble_sweep(split = [], task='empathy', post_process=True, reduction='mean', choices='all', ):\n",
    "    dir_path = f\"./{task}/dev/\"\n",
    "    gold_dev_file_path = \"/users10/zjli/workspace/WASSA/new_data/2023/dev.json\"\n",
    "    with open(gold_dev_file_path) as f:\n",
    "        gold_dev_results = json.load(f)\n",
    "    gold = [item[task] for item in gold_dev_results]\n",
    "    if split:\n",
    "        gold = [gold[i] for i in split]\n",
    "    file_names = os.listdir(dir_path)\n",
    "    file_names.sort()\n",
    "    all_pred_results = []\n",
    "    all_combined_results = []\n",
    "    final_results = []\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        if \"pearson\" in file_name:\n",
    "            pre_result = []\n",
    "            with open(os.path.join(dir_path, file_name)) as f:\n",
    "                for line in f.readlines():\n",
    "                    line = json.loads(line)\n",
    "                    if post_process:\n",
    "                        if line[0] < 1.0:\n",
    "                            line[0] = 1.0\n",
    "                        elif line[0] > 7.0:\n",
    "                            line[0] = 7.0\n",
    "                    pre_result.extend(line)\n",
    "            if choices == 'sweep' or choices == 'all':\n",
    "                all_pred_results.append({'pre_result':pre_result, 'file_name': file_name})\n",
    "            elif choices == 'MT' and 'MT' in file_name:\n",
    "                all_pred_results.append({'pre_result':pre_result, 'file_name': file_name})\n",
    "            elif choices == 'base' and 'base' in file_name:\n",
    "                all_pred_results.append({'pre_result':pre_result, 'file_name': file_name})\n",
    "\n",
    "    if choices == 'sweep':\n",
    "        first = 1\n",
    "    else:\n",
    "        first = len(all_pred_results)\n",
    "    for i in range(first, len(all_pred_results)+1): # 组合个数选取\n",
    "        for combination in combinations(all_pred_results, i): # 遍历n中选i个的所有组合\n",
    "            all_combined_results.append(combination) # 一种组合，即i组预测结果\n",
    "    for combined_result in tqdm(all_combined_results): #每一种组合\n",
    "        if reduction == 'mean':\n",
    "            ensemble_result = [pred_result['pre_result'] for pred_result in combined_result]\n",
    "            ensemble_result_array = np.mean(np.array(ensemble_result), axis=0)\n",
    "            ensemble_file_names = [pred_result['file_name'] for pred_result in combined_result]\n",
    "            \n",
    "            if split:\n",
    "                pred = [ensemble_result_array[i] for i in split]\n",
    "                pearson = calculate_pearson(gold, pred)\n",
    "                final_results.append({\n",
    "                    \"ensemble_pred_result\": ensemble_result_array,\n",
    "                    \"ensemble_file_names\": ensemble_file_names,\n",
    "                    \"ensemble_metric\": pearson\n",
    "                })\n",
    "            else:\n",
    "                pearson = calculate_pearson(gold, ensemble_result_array)\n",
    "                final_results.append({\n",
    "                    \"ensemble_pred_result\": ensemble_result_array,\n",
    "                    \"ensemble_file_names\": ensemble_file_names,\n",
    "                    \"ensemble_metric\": pearson\n",
    "                })\n",
    "    final_results = sorted(final_results, key=lambda k: k['ensemble_metric'], reverse=True)\n",
    "\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regression_ensemble_sweep(task='distress')[0]['ensemble_metric'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distress 全部数据结果\n",
    "print(regression_ensemble_sweep(task='distress', choices='all')[0]['ensemble_metric'])\n",
    "# distress 多任务数据结果\n",
    "print(regression_ensemble_sweep(task='distress', choices='MT')[0]['ensemble_metric'])\n",
    "# distress roberta-base数据结果\n",
    "print(regression_ensemble_sweep(task='distress', choices='base')[0]['ensemble_metric'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empathy 全部数据结果\n",
    "print(regression_ensemble_sweep(task='empathy', choices='all')[0]['ensemble_metric'])\n",
    "# empathy 多任务数据结果\n",
    "print(regression_ensemble_sweep(task='empathy', choices='MT')[0]['ensemble_metric'])\n",
    "# empathy roberta-base数据结果\n",
    "print(regression_ensemble_sweep(task='empathy', choices='base')[0]['ensemble_metric'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_ensemble_sweep(split = [], task='emotion', post_process=True, reduction='label_mean', choices='all'):\n",
    "    dir_path = f\"./{task}/dev\"\n",
    "    gold_dev_file_path = \"/users10/zjli/workspace/WASSA/new_data/2023/dev.json\"\n",
    "    with open(gold_dev_file_path) as f:\n",
    "        gold_dev_results = json.load(f)\n",
    "    gold = [item[task] for item in gold_dev_results]\n",
    "    if split:\n",
    "        gold = [gold[i] for i in split]\n",
    "        \n",
    "    file_names = os.listdir(dir_path)\n",
    "    file_names.sort()\n",
    "    all_pred_results = []\n",
    "    all_combined_results = []\n",
    "    final_results = []\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        if \"macro_F\" in file_name:\n",
    "            pred_prob = []\n",
    "            pred_label = []\n",
    "            with open(os.path.join(dir_path, file_name)) as f:\n",
    "                for line in f.readlines():\n",
    "                    line = json.loads(line)\n",
    "                    pred_prob.append(line['prob'])\n",
    "                    pred_label.append(line['p_label'])\n",
    "            all_pred_results.append({'pred_prob':pred_prob, 'pred_label':pred_label, 'file_name': file_name})\n",
    "    if choices == 'sweep':\n",
    "        first = 1\n",
    "    else:\n",
    "        first = len(all_pred_results)\n",
    "    for i in range(first, len(all_pred_results)+1): # 组合个数选取\n",
    "        for combination in combinations(all_pred_results, i): # 遍历n中选i个的所有组合\n",
    "            all_combined_results.append(combination) # 一种组合，即i组预测结果\n",
    "    for combined_result in tqdm(all_combined_results): #每一种组合\n",
    "        ensemble_prob_result = [pred_result['pred_prob'] for pred_result in combined_result]\n",
    "        ensemble_prob_result_array = np.array(ensemble_prob_result)\n",
    "        ensemble_prob_result_array = np.apply_along_axis(lambda x: np.mean(x), axis=0, arr=ensemble_prob_result_array)\n",
    "        if reduction == 'label_mean':\n",
    "            ensemble_label_result = [pred_result['pred_label'] for pred_result in combined_result]\n",
    "            ensemble_label_result_array = np.array(ensemble_label_result)\n",
    "            ensemble_label_result_array = np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis=0, arr=ensemble_label_result_array)\n",
    "            if post_process:\n",
    "                for idx, label_item in enumerate(ensemble_label_result_array):\n",
    "                    if sum(label_item) == 0:\n",
    "                        max_index = np.argmax(ensemble_prob_result_array[idx])\n",
    "                        ensemble_label_result_array[idx][max_index] = 1\n",
    "                        # label_item = np.zeros_like(label_item)\n",
    "                        # label_item[max_index] = 1\n",
    "            ensemble_file_names = [pred_result['file_name'] for pred_result in combined_result]\n",
    "            if split:\n",
    "                pred = [ensemble_label_result_array[i] for i in split]\n",
    "                metric = calculatePRF_MLabel(gold, pred)\n",
    "                final_results.append({\n",
    "                    \"ensemble_pred_result\": ensemble_label_result_array,\n",
    "                    \"ensemble_file_names\": ensemble_file_names,\n",
    "                    \"ensemble_metric\": metric\n",
    "                })\n",
    "            else:\n",
    "                metric = calculatePRF_MLabel(gold, ensemble_label_result_array)\n",
    "                final_results.append({\n",
    "                    \"ensemble_pred_result\": ensemble_label_result_array,\n",
    "                    \"ensemble_file_names\": ensemble_file_names,\n",
    "                    \"ensemble_metric\": metric\n",
    "                })\n",
    "        elif reduction == 'prob_mean':\n",
    "            ensemble_label_result_array = np.where(ensemble_prob_result_array > 0.5, 1, 0)\n",
    "            if post_process:\n",
    "                for idx, label_item in enumerate(ensemble_label_result_array):\n",
    "                    if sum(label_item) == 0:\n",
    "                        max_index = np.argmax(ensemble_prob_result_array[idx])\n",
    "                        ensemble_label_result_array[idx][max_index] = 1\n",
    "                        # label_item = np.zeros_like(label_item)\n",
    "                        # label_item[max_index] = 1\n",
    "            ensemble_file_names = [pred_result['file_name'] for pred_result in combined_result]\n",
    "            if split:\n",
    "                pred = [ensemble_label_result_array[i] for i in split]\n",
    "                metric = calculatePRF_MLabel(gold, pred)\n",
    "                final_results.append({\n",
    "                    \"ensemble_pred_result\": ensemble_label_result_array,\n",
    "                    \"ensemble_file_names\": ensemble_file_names,\n",
    "                    \"ensemble_metric\": metric\n",
    "                })\n",
    "            else:\n",
    "                metric = calculatePRF_MLabel(gold, ensemble_label_result_array)\n",
    "                final_results.append({\n",
    "                    \"ensemble_pred_result\": ensemble_label_result_array,\n",
    "                    \"ensemble_file_names\": ensemble_file_names,\n",
    "                    \"ensemble_metric\": metric\n",
    "                })\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "    final_results = sorted(final_results, key=lambda k: k['ensemble_metric'], reverse=True)\n",
    "\n",
    "    return final_results\n",
    "    # final_results = sorted(final_results, key=lambda k: k['ensemble_metric'], reverse=True)\n",
    "    # print(final_results[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_mean 全部数据结果\n",
    "print(classification_ensemble_sweep(split='dev', task='emotion', choices='all', reduction='label_mean')[0]['ensemble_metric'])\n",
    "# prob_mean 全部数据结果\n",
    "print(classification_ensemble_sweep(split='dev', task='emotion', choices='all', reduction='prob_mean')[0]['ensemble_metric'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 划分验证集评估以验证不同ensemble的鲁棒性"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "期望：在不同的划分上，模型的效果近似，而不是偏向于某一种划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/users10/zjli/workspace/WASSA/new_data/2023/WASSA23_essay_level_dev.tsv\", delimiter='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n",
      "163\n",
      "292\n",
      "##########################################\n",
      "1\n",
      "1\n",
      "2\n",
      "##########################################\n",
      "25\n",
      "29\n",
      "33\n",
      "##########################################\n",
      "3\n",
      "4\n",
      "6\n",
      "##########################################\n",
      "25000\n",
      "30000\n",
      "55000\n"
     ]
    }
   ],
   "source": [
    "print(sorted(list(df['article_id']))[len(df)//4])\n",
    "print(sorted(list(df['article_id']))[len(df)*2//4])\n",
    "print(sorted(list(df['article_id']))[len(df)*3//4])\n",
    "print('##########################################')\n",
    "print(sorted(list(df['gender']))[len(df)//4])\n",
    "print(sorted(list(df['gender']))[len(df)*2//4])\n",
    "print(sorted(list(df['gender']))[len(df)*3//4])\n",
    "print('##########################################')\n",
    "print(sorted(list(df['age']))[len(df)//4])\n",
    "print(sorted(list(df['age']))[len(df)*2//4])\n",
    "print(sorted(list(df['age']))[len(df)*3//4])\n",
    "print('##########################################')\n",
    "print(sorted(list(df['education']))[len(df)//4])\n",
    "print(sorted(list(df['education']))[len(df)*2//4])\n",
    "print(sorted(list(df['education']))[len(df)*3//4])\n",
    "print('##########################################')\n",
    "print(sorted(list(df['income']))[len(df)//4])\n",
    "print(sorted(list(df['income']))[len(df)*2//4])\n",
    "print(sorted(list(df['income']))[len(df)*3//4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dev(df, key, start, end):\n",
    "    res = []\n",
    "    for idx, item in df[key].items():\n",
    "        if start <= item <= end:\n",
    "            res.append(idx)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_split_metric(pred, gold, split_indexs, task):\n",
    "    s_pred = [pred[idx] for idx in split_indexs]\n",
    "    s_gold = [gold[idx] for idx in split_indexs]\n",
    "    if task == 'emotion':\n",
    "        metric = calculatePRF_MLabel(s_gold, s_pred)\n",
    "    elif task in ['empathy', 'distress']:\n",
    "        metric = calculate_pearson(s_gold, s_pred)\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/users10/zjli/miniconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1592: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "gold_dev_file_path = \"/users10/zjli/workspace/WASSA/new_data/2023/dev.json\"\n",
    "\n",
    "\n",
    "key_se = {\"article_id\":[(0,73),(74,163),(164,292),(293,1000000000000)], \n",
    "          \"age\":[(0,25),(26,29),(30,33),(34,10000000000000)],\n",
    "        \"education\":[(0,3),(4,4),(5,6),(7,1000000000000)],\n",
    "        \"income\":[(0,25000),(25001,30000),(30001,55000),(55001,1000000000000)]}\n",
    "\n",
    "for task in ['emotion', 'empathy', 'distress']:\n",
    "    with open(gold_dev_file_path) as f:\n",
    "        gold_dev_results = json.load(f)\n",
    "        gold = [item[task] for item in gold_dev_results]\n",
    "    file_names = os.listdir(f'./{task}/dev/')\n",
    "    file_names.sort()\n",
    "    all_pred_results = []\n",
    "    all_combined_results = []\n",
    "    final_results = []\n",
    "    metrics = []\n",
    "    for file_name in file_names:\n",
    "        if \"macro_F\" in file_name or 'pearson' in file_name:\n",
    "            pred_prob = []\n",
    "            pred_label = []\n",
    "            with open(os.path.join(f'./{task}/dev/', file_name)) as f:\n",
    "                for line in f.readlines():\n",
    "                    line = json.loads(line)\n",
    "                    if task == 'emotion':\n",
    "                        if sum(line['p_label']) == 0:\n",
    "                            max_index = np.argmax(line['prob'])\n",
    "                            tmp = [0]*8\n",
    "                            tmp[max_index] = 1\n",
    "                        else:\n",
    "                            tmp = line['p_label']\n",
    "                        pred_label.append(tmp)\n",
    "                    elif task in ['empathy', 'distress']:\n",
    "                        if line[0] < 1:\n",
    "                            val = 1.0\n",
    "                        elif line[0] > 7:\n",
    "                            val = 7.0\n",
    "                        else:\n",
    "                            val = line[0]\n",
    "                        pred_label.append(val)\n",
    "            row_metric = {'file_name':file_name}\n",
    "            for key in [\"article_id\", \"age\", \"education\", \"income\"]:\n",
    "                for idx, (start, end) in enumerate(key_se[key]):\n",
    "                    split_indexs = split_dev(df, key, start, end)\n",
    "                    metric = dev_split_metric(pred_label, gold, split_indexs, task)\n",
    "                    row_metric[f'{key}_{idx}'] = metric\n",
    "            metrics.append(row_metric)\n",
    "    wdf = pd.DataFrame(metrics)\n",
    "    # 将 DataFrame 写入 TSV 文件\n",
    "    wdf.to_csv(f'./{task}/contrast_result.tsv', sep='\\t', index=False)\n",
    "                            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 按照 article_id划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4\n",
      "-12\n",
      "-40\n",
      "-34\n",
      "-2\n"
     ]
    }
   ],
   "source": [
    "article_id_set_1 = []\n",
    "article_id_set_2 = []\n",
    "for idx, item in df['article_id'].items():\n",
    "    if item <= 163:\n",
    "        article_id_set_1.append(idx)\n",
    "    else:\n",
    "        article_id_set_2.append(idx)\n",
    "print(len(article_id_set_2) - len(article_id_set_1))\n",
    "gender_set_1 = []\n",
    "gender_set_2 = []\n",
    "for idx, item in df['gender'].items():\n",
    "    if item == 1:\n",
    "        gender_set_1.append(idx)\n",
    "    else:\n",
    "        gender_set_2.append(idx)\n",
    "print(len(gender_set_2) - len(gender_set_1))\n",
    "age_set_1 = []\n",
    "age_set_2 = []\n",
    "for idx, item in df['age'].items():\n",
    "    if item <= 29:\n",
    "        age_set_1.append(idx)\n",
    "    else:\n",
    "        age_set_2.append(idx)\n",
    "print(len(age_set_2) - len(age_set_1))\n",
    "education_set_1 = []\n",
    "education_set_2 = []\n",
    "for idx, item in df['education'].items():\n",
    "    if item <= 4:\n",
    "        education_set_1.append(idx)\n",
    "    else:\n",
    "        education_set_2.append(idx)\n",
    "print(len(education_set_2) - len(education_set_1))\n",
    "income_set_1 = []\n",
    "income_set_2 = []\n",
    "for idx, item in df['income'].items():\n",
    "    if item <= 30000:\n",
    "        income_set_1.append(idx)\n",
    "    else:\n",
    "        income_set_2.append(idx)\n",
    "print(len(income_set_2) - len(income_set_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_set_splits = {\"article_id_set_1\": article_id_set_1,\n",
    "                  \"article_id_set_2\": article_id_set_2,\n",
    "                  \"gender_set_1\": gender_set_1,\n",
    "                  \"gender_set_2\": gender_set_2,\n",
    "                  \"age_set_1\": age_set_1,\n",
    "                  \"age_set_2\": age_set_2,\n",
    "                  \"education_set_1\": education_set_1,\n",
    "                  \"education_set_2\": education_set_2,\n",
    "                  \"income_set_1\": income_set_1,\n",
    "                  \"income_set_2\": income_set_2,\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a831cefc60f74c9b9586e764a3f8ea2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/524287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m split_name \u001b[39min\u001b[39;00m dev_set_splits\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m      3\u001b[0m     split_set \u001b[39m=\u001b[39m dev_set_splits[split_name]\n\u001b[0;32m----> 4\u001b[0m     empathy_result \u001b[39m=\u001b[39m regression_ensemble_sweep(split\u001b[39m=\u001b[39;49msplit_set, choices\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msweep\u001b[39;49m\u001b[39m'\u001b[39;49m, task\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mempathy\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m     distress_result \u001b[39m=\u001b[39m regression_ensemble_sweep(split\u001b[39m=\u001b[39msplit_set, choices\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msweep\u001b[39m\u001b[39m'\u001b[39m, task\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdistress\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m     emotion_result \u001b[39m=\u001b[39m classification_ensemble_sweep(split\u001b[39m=\u001b[39msplit_set, choices\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msweep\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 49\u001b[0m, in \u001b[0;36mregression_ensemble_sweep\u001b[0;34m(split, task, post_process, reduction, choices)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m split:\n\u001b[1;32m     48\u001b[0m     pred \u001b[39m=\u001b[39m [ensemble_result_array[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m split]\n\u001b[0;32m---> 49\u001b[0m     pearson \u001b[39m=\u001b[39m calculate_pearson(gold, pred)\n\u001b[1;32m     50\u001b[0m     final_results\u001b[39m.\u001b[39mappend({\n\u001b[1;32m     51\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mensemble_pred_result\u001b[39m\u001b[39m\"\u001b[39m: ensemble_result_array,\n\u001b[1;32m     52\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mensemble_file_names\u001b[39m\u001b[39m\"\u001b[39m: ensemble_file_names,\n\u001b[1;32m     53\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mensemble_metric\u001b[39m\u001b[39m\"\u001b[39m: pearson\n\u001b[1;32m     54\u001b[0m     })\n\u001b[1;32m     55\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/workspace/WASSA/ensemble/main.py:49\u001b[0m, in \u001b[0;36mcalculate_pearson\u001b[0;34m(gold, prediction)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39mgold/prediction are a list of lists [ emp pred , distress pred ]\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39m# converting to float\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m gold \u001b[39m=\u001b[39m [\u001b[39mfloat\u001b[39m(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m gold]\n\u001b[1;32m     50\u001b[0m prediction \u001b[39m=\u001b[39m [\u001b[39mfloat\u001b[39m(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m prediction]\n\u001b[1;32m     52\u001b[0m \u001b[39mreturn\u001b[39;00m pearsonr(gold, prediction)\n",
      "File \u001b[0;32m~/workspace/WASSA/ensemble/main.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39mgold/prediction are a list of lists [ emp pred , distress pred ]\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39m# converting to float\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m gold \u001b[39m=\u001b[39m [\u001b[39mfloat\u001b[39;49m(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m gold]\n\u001b[1;32m     50\u001b[0m prediction \u001b[39m=\u001b[39m [\u001b[39mfloat\u001b[39m(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m prediction]\n\u001b[1;32m     52\u001b[0m \u001b[39mreturn\u001b[39;00m pearsonr(gold, prediction)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "emp_results, dis_results, emo_results = {}, {}, {}\n",
    "for split_name in dev_set_splits.keys():\n",
    "    split_set = dev_set_splits[split_name]\n",
    "    empathy_result = regression_ensemble_sweep(split=split_set, choices='sweep', task='empathy')\n",
    "    distress_result = regression_ensemble_sweep(split=split_set, choices='sweep', task='distress')\n",
    "    emotion_result = classification_ensemble_sweep(split=split_set, choices='sweep')\n",
    "    emp_results[split_name] = [item['ensemble_file_names'] for item in empathy_result[:100]]\n",
    "    dis_results[split_name] = [item['ensemble_file_names'] for item in distress_result[:100]]\n",
    "    emo_results[split_name] = [item['ensemble_file_names'] for item in emotion_result[:100]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./result/empathy_100.json\", 'w') as f:\n",
    "    json.dump(emp_results, f)\n",
    "with open(\"./result/distress_100.json\", 'w') as f:\n",
    "    json.dump(dis_results, f)\n",
    "with open(\"./result/emotion_100.json\", 'w') as f:\n",
    "    json.dump(emo_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_results, empathy_results, distress_results = {}, {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result1, final_result2 = classification_ensemble_sweep(split=[article_id_set_1, article_id_set_2], choices='sweep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result1, final_result2 = regression_ensemble_sweep(split=[article_id_set_1, article_id_set_2], choices='sweep', task='empathy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result1, final_result2 = regression_ensemble_sweep(split=[article_id_set_1, article_id_set_2], choices='sweep', task='distress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distress: 678\n",
      "[(['MT_1_pearson_0.626.json', 'roberta-base_0_pearson_0.6346.json'],\n",
      "  [100000, 90, 4, 100000, 42, 42, 100000, 1, 100000, 0]),\n",
      " (['MT_1_pearson_0.626.json',\n",
      "   'roberta-base_0_pearson_0.6346.json',\n",
      "   'roberta-base_9_pearson_0.606.json'],\n",
      "  [100000, 100000, 7, 100000, 100000, 9, 100000, 4, 100000, 2]),\n",
      " (['MT_0_pearson_0.6059.json',\n",
      "   'MT_3_pearson_0.5949.json',\n",
      "   'roberta-base_0_pearson_0.6346.json',\n",
      "   'roberta-base_7_pearson_0.6047.json'],\n",
      "  [13, 100000, 100000, 100000, 3, 100000, 9, 100000, 6, 100000]),\n",
      " (['MT_0_pearson_0.6059.json',\n",
      "   'MT_2_pearson_0.6052.json',\n",
      "   'MT_3_pearson_0.5949.json',\n",
      "   'roberta-base_0_pearson_0.6346.json'],\n",
      "  [100000, 100000, 100000, 15, 7, 100000, 28, 100000, 2, 100000]),\n",
      " (['MT_0_pearson_0.6059.json',\n",
      "   'MT_3_pearson_0.5949.json',\n",
      "   'roberta-base_0_pearson_0.6346.json'],\n",
      "  [100000, 100000, 100000, 54, 0, 100000, 4, 100000, 1, 100000])]\n",
      "empathy: 930\n",
      "[(['MT_3_pearson_0.6322.json',\n",
      "   'roberta-base_14_pearson_0.6367.json',\n",
      "   'roberta-base_9_pearson_0.6433.json'],\n",
      "  [100000, 100000, 1, 100000, 100000, 11, 4, 100000, 6, 100000]),\n",
      " (['MT_3_pearson_0.6322.json',\n",
      "   'roberta-base_14_pearson_0.6367.json',\n",
      "   'roberta-base_15_pearson_0.6346.json',\n",
      "   'roberta-base_9_pearson_0.6433.json'],\n",
      "  [100000, 100000, 21, 100000, 100000, 100000, 34, 100000, 10, 100000]),\n",
      " (['MT_3_pearson_0.6322.json',\n",
      "   'roberta-base_14_pearson_0.6367.json',\n",
      "   'roberta-base_3_pearson_0.6224.json'],\n",
      "  [100000, 41, 23, 100000, 100000, 100000, 5, 100000, 100000, 100000]),\n",
      " (['MT_3_pearson_0.6322.json',\n",
      "   'roberta-base_14_pearson_0.6367.json',\n",
      "   'roberta-base_1_pearson_0.6309.json'],\n",
      "  [100000, 15, 71, 100000, 100000, 7, 100000, 100000, 100000, 100000]),\n",
      " (['MT_3_pearson_0.6322.json',\n",
      "   'roberta-base_14_pearson_0.6367.json',\n",
      "   'roberta-base_1_pearson_0.6309.json',\n",
      "   'roberta-base_3_pearson_0.6224.json',\n",
      "   'roberta-base_9_pearson_0.6433.json'],\n",
      "  [100000, 100000, 9, 100000, 100000, 100000, 43, 100000, 100000, 53])]\n",
      "emotion: 815\n",
      "[(['14_macro_F_0.6087.json', '7_macro_F_0.5964.json', '9_macro_F_0.6178.json'],\n",
      "  [2, 100000, 100000, 1, 10, 100000, 0, 100000, 4, 100000]),\n",
      " (['14_macro_F_0.6087.json', '1_macro_F_0.5953.json', '9_macro_F_0.6178.json'],\n",
      "  [13, 100000, 100000, 5, 7, 100000, 12, 100000, 85, 100000]),\n",
      " (['0_macro_F_0.5851.json',\n",
      "   '11_macro_F_0.6095.json',\n",
      "   '13_macro_F_0.5891.json',\n",
      "   '14_macro_F_0.6087.json',\n",
      "   '2_macro_F_0.6098.json',\n",
      "   '3_macro_F_0.6087.json',\n",
      "   '6_macro_F_0.588.json',\n",
      "   '8_macro_F_0.5852.json',\n",
      "   '9_macro_F_0.6178.json'],\n",
      "  [100000, 10, 45, 100000, 100000, 1, 100000, 80, 100000, 100000]),\n",
      " (['0_macro_F_0.5851.json',\n",
      "   '11_macro_F_0.6095.json',\n",
      "   '13_macro_F_0.5891.json',\n",
      "   '14_macro_F_0.6087.json',\n",
      "   '3_macro_F_0.6087.json',\n",
      "   '6_macro_F_0.588.json',\n",
      "   '9_macro_F_0.6178.json'],\n",
      "  [100000, 2, 13, 100000, 100000, 32, 94, 100000, 100000, 100000]),\n",
      " (['0_macro_F_0.5851.json',\n",
      "   '11_macro_F_0.6095.json',\n",
      "   '14_macro_F_0.6087.json',\n",
      "   '2_macro_F_0.6098.json',\n",
      "   '3_macro_F_0.6087.json',\n",
      "   '6_macro_F_0.588.json',\n",
      "   '9_macro_F_0.6178.json'],\n",
      "  [100000, 100000, 20, 100000, 100000, 55, 100000, 76, 100000, 4])]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "tasks = ['distress', 'empathy', 'emotion']\n",
    "\n",
    "for task in tasks:\n",
    "    with open(f'./result/{task}_100.json') as f:\n",
    "        data = json.load(f)\n",
    "        candidate_file_name_groups = set()\n",
    "        candidate_file_name_weights = dict()\n",
    "        for file_name_groups in data.values(): # 每个dev\n",
    "            for file_name_group in file_name_groups: # 前100的模型预测组合\n",
    "                candidate_file_name_groups.add(str(file_name_group))\n",
    "        print(f\"{task}: {len(candidate_file_name_groups)}\")\n",
    "        candidate_file_name_weights = {item:[] for item in candidate_file_name_groups}\n",
    "        for file_name_groups in data.values():\n",
    "            str_file_name_groups = [str(item) for item in file_name_groups]\n",
    "            for cfng in candidate_file_name_groups:\n",
    "                if cfng in str_file_name_groups:\n",
    "                    candidate_file_name_weights[cfng].append(str_file_name_groups.index(cfng))\n",
    "                else:\n",
    "                    candidate_file_name_weights[cfng].append(100000)\n",
    "        candidate_file_name_weights = sorted(candidate_file_name_weights.items(), key=lambda k: sum(k[1]))\n",
    "        pprint([(eval(item[0]), item[1]) for item in candidate_file_name_weights[:5]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 按照label划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gold_dev_file_path = \"/users10/zjli/workspace/WASSA/new_data/2023/dev.json\"\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "# key_se = {\"emotion\":[(0,73),(74,163),(164,292),(293,1000000000000)], \n",
    "#           \"age\":[(0,25),(26,29),(30,33),(34,10000000000000)],\n",
    "#         \"education\":[(0,3),(4,4),(5,6),(7,1000000000000)],\n",
    "#         \"income\":[(0,25000),(25001,30000),(30001,55000),(55001,1000000000000)]}\n",
    "\n",
    "for task in ['emotion']:\n",
    "    with open(gold_dev_file_path) as f:\n",
    "        gold_dev_results = json.load(f)\n",
    "        gold = [item[task] for item in gold_dev_results]\n",
    "    file_names = os.listdir(f'./{task}/dev/')\n",
    "    file_names.sort()\n",
    "    all_pred_results = []\n",
    "    all_combined_results = []\n",
    "    final_results = []\n",
    "    metrics = []\n",
    "    for file_name in file_names:\n",
    "        if \"macro_F\" in file_name or 'pearson' in file_name:\n",
    "            pred_prob = []\n",
    "            pred_label = []\n",
    "            with open(os.path.join(f'./{task}/dev/', file_name)) as f:\n",
    "                for line in f.readlines():\n",
    "                    line = json.loads(line)\n",
    "                    if task == 'emotion':\n",
    "                        if sum(line['p_label']) == 0:\n",
    "                            max_index = np.argmax(line['prob'])\n",
    "                            tmp = [0]*8\n",
    "                            tmp[max_index] = 1\n",
    "                        else:\n",
    "                            tmp = line['p_label']\n",
    "                        pred_label.append(tmp)\n",
    "                    elif task in ['empathy', 'distress']:\n",
    "                        if line[0] < 1:\n",
    "                            val = 1.0\n",
    "                        elif line[0] > 7:\n",
    "                            val = 7.0\n",
    "                        else:\n",
    "                            val = line[0]\n",
    "                        pred_label.append(val)\n",
    "            row_metric = {'file_name':file_name}\n",
    "            report = classification_report(gold, pred_label, output_dict=True)\n",
    "            for i in range(8):\n",
    "                row_metric[f'{key}_{i}'] = report[str(i)]['f1-score']\n",
    "            metrics.append(row_metric)\n",
    "    wdf = pd.DataFrame(metrics)\n",
    "    # 将 DataFrame 写入 TSV 文件\n",
    "    wdf.to_csv(f'./{task}/emo_contrast_result.tsv', sep='\\t', index=False)\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dev(df, emo_idx):\n",
    "    res = []\n",
    "    for idx, item in enumerate(df):\n",
    "        if item[emo_idx] == 1:\n",
    "            res.append(idx)\n",
    "    return res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
